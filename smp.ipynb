{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libs\n",
    "# !python -m pip install --user numpy matplotlib ipython ipykernel jupyter segmentation-models-pytorch albumentations opencv-python tqdm natsort gdown scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHP dataset link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download LV-MHP-v2 \n",
    "# https://drive.google.com/file/d/1YVBGMru0dlwB8zu1OoErOazZoc8ISSJn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to revert mtime: /Library/Fonts\n",
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to visualize images, masks and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for data visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHP Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from natsort import natsorted\n",
    "from functools import reduce\n",
    "from skimage import io\n",
    "\n",
    "def verify_image(img_file):\n",
    "    try:\n",
    "        img = io.imread(img_file)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_mask_fp_infos(mask_fp):\n",
    "    id, num_persons, curr_person = os.path.basename(mask_fp).split(\".\")[0].split(\"_\")\n",
    "    return int(id), int(num_persons), int(curr_person)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHP Dataloader\n",
    "\n",
    "Writing helper class for data extraction, tranformation and preprocessing  \n",
    "https://pytorch.org/docs/stable/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    \"\"\"MHP v2 Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    CLASS_LABELS = [\n",
    "        \"Background\", \"Cap/hat\",\"Helmet\", \"Face\", \"Hair\", \"Left-arm\", \"Right-arm\", \"Left-hand\", \"Right-hand\", \"Protector\", \n",
    "        \"Bikini/bra\", \"Jacket/windbreaker/hoodie\", \"Tee-shirt\", \"Polo-shirt\", \"Sweater\", \"Singlet\", \"Torso-skin\", \n",
    "        \"Pants\", \"Shorts/swim-shorts\", \"Skirt\", \"Stockings\", \"Socks\", \"Left-boot\", \"Right-boot\", \"Left-shoe\",\n",
    "        \"Right-shoe\", \"Left-highheel\", \"Right-highheel\", \"Left-sandal\", \"Right-sandal\", \"Left-leg\", \"Right-leg\",\n",
    "        \"Left-foot\", \"Right-foot\", \"Coat\", \"Dress\", \"Robe\", \"Jumpsuit\" , \"Other-full-body-clothes\" , \"Headwear\",\n",
    "        \"Backpack\", \"Ball\", \"Bats\", \"Belt\", \"Bottle\", \"Carrybag\", \"Cases\", \"Sunglasses\", \"Eyewear\", \"Glove\",\n",
    "        \"Scarf\", \"Umbrella\",\" Wallet/purse\", \"Watch\", \"Wristband\", \"Tie\",\" Other-accessary\", \n",
    "        \"Other-upper-body-clothes\", \"Other-lower-body-clothes\"\n",
    "        ]\n",
    "\n",
    "    def __init__(self, size, classes=None, augmentation=None, preprocessing=None):\n",
    "        images_dir = \"data/LV-MHP-v2/train/images/\"\n",
    "        masks_dir = \"data/LV-MHP-v2/train/parsing_annos/\"\n",
    "        self.images_ids = natsorted(os.listdir(images_dir))[0:size]\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.images_ids]\n",
    "        self.masks_ids = natsorted(os.listdir(masks_dir))\n",
    "        self.masks_fps = [os.path.join(masks_dir, mask_id) for mask_id in self.masks_ids]\n",
    "    \n",
    "        # convert str names to class values on masks\n",
    "        self.class_indices = [self.CLASS_LABELS.index(cls) for cls in classes]\n",
    "\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if verify_image(self.images_fps[i]):\n",
    "            # read image\n",
    "            image = cv2.imread(self.images_fps[i])\n",
    "        else:\n",
    "            image = cv2.imread(self.images_fps[i-1])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # combine mask files\n",
    "        mask_files_per_image = []\n",
    "        for el in self.masks_fps:\n",
    "            mask_id = int(get_mask_fp_infos(el)[0]) \n",
    "            img_id = int(self.images_ids[i].split(\".\")[0])\n",
    "            if(mask_id == img_id):\n",
    "                mask_files_per_image.append(el)\n",
    "        \n",
    "        mask_files = []\n",
    "        for file in mask_files_per_image:\n",
    "            mask_file = cv2.imread(file)\n",
    "            if mask_file.ndim == 3:\n",
    "                mask_file = cv2.cvtColor(mask_file, cv2.COLOR_BGR2RGB)\n",
    "                mask_file = mask_file[:,:,0]\n",
    "                mask_files.append(mask_file)\n",
    "        # Create union of all mask files\n",
    "        combined_file = reduce(lambda array_a, array_b: array_a | array_b, mask_files)\n",
    "\n",
    "         # extract certain classes from mask (e.g. cars)\n",
    "        masks = [(combined_file == label_index) for label_index in self.class_indices]\n",
    "        mask = reduce(lambda array_a, array_b: array_a | array_b, masks)\n",
    "        if self.class_indices == [0]:\n",
    "            mask = np.invert(mask)\n",
    "        mask = mask[:,:, np.newaxis]\n",
    "        mask = mask.astype('float')\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask, combined_file=combined_file)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/LV-MHP-v2/train/images/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2j/ks2cqvx92q11pm4w2c0j397h0000gn/T/ipykernel_17165/2153680922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Background'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexample_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get some sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m visualize(\n",
      "\u001b[0;32m/var/folders/2j/ks2cqvx92q11pm4w2c0j397h0000gn/T/ipykernel_17165/2387054794.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, classes, augmentation, preprocessing)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mimages_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/LV-MHP-v2/train/images/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mmasks_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/LV-MHP-v2/train/parsing_annos/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_fps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/LV-MHP-v2/train/images/'"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(size=2000, classes=['Background'])\n",
    "\n",
    "example_image, example_mask = dataset[0] # get some sample\n",
    "\n",
    "visualize(\n",
    "    image=example_image, \n",
    "    mask=example_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VOCSegmentation\n",
    "import albumentations as albu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = [\n",
    "    \"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \n",
    "    \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \n",
    "    \"person\", \"potted plant\", \"sheep\", \"sofa\", \"train\", \"tv/monitor\",\n",
    "]\n",
    "\n",
    "VOC_COLORMAP = [\n",
    "    [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128], \n",
    "    [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128], \n",
    "    [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0], [0, 64, 128],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedVOC(VOCSegmentation):\n",
    "    # def __init__(self, augmentation=None, preprocessing=None, classes=VOC_CLASSES):\n",
    "    def __init__(self, root, image_set,  augmentation=None, preprocessing=None):\n",
    "        super(TransformedVOC, self).__init__(root, image_set = image_set)\n",
    "        # self.classes = classes\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _convert_to_segmentation_mask(mask, labels):\n",
    "    #     height, width = mask.shape[:2]\n",
    "    #     segmentation_mask = np.zeros((height, width, len(labels)), dtype=np.float32)\n",
    "    #     for idx, label in enumerate(labels):\n",
    "    #         label_idx = VOC_CLASSES.index(label)\n",
    "    #         label_color = VOC_COLORMAP[label_idx]\n",
    "    #         segmentation_mask[:, :, idx] = np.all(mask == label_color, axis=-1).astype(float)\n",
    "    #     return segmentation_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # image = cv2.imread(self.images[idx])\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # mask = cv2.imread(self.masks[idx])\n",
    "        # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        # mask = self._convert_to_segmentation_mask(mask, self.classes)\n",
    "\n",
    "        image, mask = super(TransformedVOC, self).__getitem__(idx)\n",
    "        image = np.array(image)\n",
    "        mask = np.array(mask)\n",
    "        mask = mask.astype('float')\n",
    "        mask = np.where(mask==15, 1., 0.)\n",
    "        uniques = np.unique(mask)\n",
    "        if 1. not in uniques:\n",
    "            remove_list.append(idx)\n",
    "        mask = mask[:,:,np.newaxis]\n",
    "\n",
    "         # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TransformedVOC(root=\"data/\", image_set=\"trainval\", classes=[\"person\"])\n",
    "dataset = TransformedVOC(root=\"data/\", image_set=\"trainval\")\n",
    "\n",
    "# dataset = VOCSegmentation(root=\"data/\", image_set=\"trainval\")\n",
    "\n",
    "example_image, example_mask = dataset[0] # get some sample\n",
    "\n",
    "visualize(\n",
    "    image=example_image, \n",
    "    mask=example_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOC Orig Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCOrig(VOCSegmentation):\n",
    "    def __init__(self, root, image_set,  augmentation=None, preprocessing=None, classes=VOC_CLASSES):\n",
    "        super(VOCOrig, self).__init__(root, image_set = image_set)\n",
    "        self.classes = classes\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        \n",
    "    @staticmethod\n",
    "    def _convert_to_segmentation_mask(mask, labels):\n",
    "        height, width = mask.shape[:2]\n",
    "        segmentation_mask = np.zeros((height, width, len(labels)), dtype=np.float32)\n",
    "        for idx, label in enumerate(labels):\n",
    "            label_idx = VOC_CLASSES.index(label)\n",
    "            label_color = VOC_COLORMAP[label_idx]\n",
    "            segmentation_mask[:, :, idx] = np.all(mask == label_color, axis=-1).astype(float)\n",
    "        return segmentation_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = cv2.imread(self.images[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks[idx])\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = self._convert_to_segmentation_mask(mask, self.classes)\n",
    "\n",
    "         # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC Orig Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = TransformedVOC(root=\"data/\", image_set=\"trainval\", classes=[\"person\"])\n",
    "dataset = VOCOrig(root=\"data/\", image_set=\"trainval\")\n",
    "\n",
    "example_image, example_mask = dataset[0] # get some sample\n",
    "\n",
    "    \n",
    "visualize(\n",
    "    image=example_image, \n",
    "    mask0=example_mask[:,:,0],\n",
    "    mask1=example_mask[:,:,1],\n",
    "    mask2=example_mask[:,:,2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation is a powerful technique to increase the amount of your data and prevent model overfitting.  \n",
    "If you not familiar with such trick read some of these articles:\n",
    " - [The Effectiveness of Data Augmentation in Image Classification using Deep\n",
    "Learning](http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf)\n",
    " - [Data Augmentation | How to use Deep Learning when you have Limited Data](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced)\n",
    " - [Data Augmentation Experimentation](https://towardsdatascience.com/data-augmentation-experimentation-3e274504f04b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "IMG_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MHP Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadCrop(object):\n",
    "    \"\"\"Crops the given PIL Image around head region.\n",
    "\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size of the crop. If size is an\n",
    "            int instead of sequence like (h, w), a square crop (size, size) is\n",
    "            made.\n",
    "        offset (int): Offset in y-axis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, offset):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "        self.offset = offset\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        image = kwargs.get('image', None)\n",
    "        mask = kwargs.get('mask', None)\n",
    "        combined_file = kwargs.get('combined_file', None)\n",
    "        \n",
    "\n",
    "        # Extract head region\n",
    "        head_masks = [(combined_file == label_index) for label_index in [4, 3, 2, 39]]\n",
    "        head_mask = reduce(lambda array_a, array_b: array_a | array_b, head_masks)\n",
    "        # Get coordinates of first head pixel\n",
    "        first_head = np.where(head_mask == True)\n",
    "        row, col = first_head[0][0], first_head[1][0]\n",
    "        x_min = max(0, col - self.size[1] // 2)\n",
    "        y_min = max(0, row + self.offset - self.size[0] // 2)\n",
    "        x_max = x_min + self.size[1]\n",
    "        y_max = y_min + self.size[0]\n",
    "        cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "        cropped_mask = mask[y_min:y_max, x_min:x_max]\n",
    "        return {\"image\": cropped_image, \"mask\": cropped_mask}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "class CustomResize(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        image = kwargs.get('image', None)\n",
    "        mask = kwargs.get('mask', None)\n",
    "        image = cv2.resize(image, (self.size, self.size), interpolation=cv2.INTER_NEAREST)\n",
    "        mask = cv2.resize(mask, (self.size, self.size), interpolation=cv2.INTER_NEAREST)\n",
    "        return {\"image\": image, \"mask\": mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "        HeadCrop((200, 400), 100),\n",
    "        albu.Resize(IMG_SIZE,IMG_SIZE)\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    test_transform = [\n",
    "        HeadCrop((200, 400), 100),\n",
    "        albu.Resize(IMG_SIZE,IMG_SIZE)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MHP Augmented Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    augmentation=get_training_augmentation(),\n",
    "    size=2000, \n",
    "    classes=['Background'])\n",
    "\n",
    "example_image, example_mask = dataset[0] # get some sample\n",
    "\n",
    "visualize(\n",
    "    image=example_image, \n",
    "    mask=example_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VOC Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation_voc():\n",
    "    train_transform = [\n",
    "        albu.Resize(IMG_SIZE,IMG_SIZE)\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def get_validation_augmentation_voc():\n",
    "    test_transform = [\n",
    "        albu.Resize(IMG_SIZE,IMG_SIZE)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VOC Augmented Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TransformedVOC(root=\"data/\", image_set=\"trainval\", augmentation=get_training_augmentation_voc())\n",
    "\n",
    "example_image, example_mask = dataset[0] # get some sample\n",
    "\n",
    "visualize(\n",
    "    image=example_image, \n",
    "    mask=example_mask,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOC Orig Augmented Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_voc_orig = VOCOrig(\n",
    "    root=\"data/\", \n",
    "    image_set=\"train\", \n",
    "    augmentation=get_training_augmentation_voc(), \n",
    "    )\n",
    "\n",
    "example_image, example_mask = train_dataset_voc_orig[0] # get some sample\n",
    "\n",
    "\n",
    "visualize(\n",
    "    image=example_image, \n",
    "    mask0=example_mask[:,:,0],\n",
    "    mask1=example_mask[:,:,1],\n",
    "    mask2=example_mask[:,:,2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'timm-mobilenetv3_small_minimal_100'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['Background']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model1 = smp.DeepLabV3Plus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model2 = smp.DeepLabV3(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model3 = smp.DeepLabV3(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=21, \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MHP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "dataset = Dataset(\n",
    "    size=1464+1449, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn), \n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size =  int(np.floor(dataset_size*0.5))\n",
    "valid_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VOC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "\n",
    "train_dataset_voc = TransformedVOC(\n",
    "    root=\"data/\", \n",
    "    image_set=\"train\", \n",
    "    augmentation=get_training_augmentation_voc(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    "    )\n",
    "    \n",
    "valid_dataset_voc = TransformedVOC(\n",
    "    root=\"data/\", \n",
    "    image_set=\"val\", \n",
    "    augmentation=get_validation_augmentation_voc(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn))\n",
    "\n",
    "train_loader_voc = DataLoader(train_dataset_voc, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader_voc = DataLoader(valid_dataset_voc, batch_size=1, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VOC orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_voc_orig = VOCOrig(\n",
    "    root=\"data/\", \n",
    "    image_set=\"train\", \n",
    "    augmentation=get_training_augmentation_voc(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    "    )\n",
    "\n",
    "\n",
    "valid_dataset_voc_orig = VOCOrig(\n",
    "    root=\"data/\", \n",
    "    image_set=\"val\", \n",
    "    augmentation=get_validation_augmentation_voc(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    "    )\n",
    "\n",
    "train_loader_voc_orig = DataLoader(train_dataset_voc_orig, batch_size=8, shuffle=True, num_workers=0)\n",
    "valid_loader_voc_orig = DataLoader(valid_dataset_voc_orig, batch_size=1, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "ALPHA = 0.7\n",
    "BETA = 0.3\n",
    "GAMMA = 0.75\n",
    "\n",
    "class FocalTverskyLoss(nn.Module):\n",
    "    __name__ = 'focal_tversky_loss'\n",
    "\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(FocalTverskyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = torch.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #True Positives, False Positives & False Negatives\n",
    "        TP = (inputs * targets).sum()  \n",
    "        FP = ((1-targets) * inputs).sum()\n",
    "        FN = (targets * (1-inputs)).sum()\n",
    "        \n",
    "        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "        FocalTversky = (1 - Tversky)**gamma\n",
    "        \n",
    "        return FocalTversky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = smp.utils.losses.DiceLoss()\n",
    "loss = FocalTverskyLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "    smp.utils.metrics.Fscore()\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model1.parameters(), lr=0.0001),\n",
    "])\n",
    "\n",
    "optimizer2 = torch.optim.Adam([ \n",
    "    dict(params=model2.parameters(), lr=0.0001),\n",
    "])\n",
    "\n",
    "optimizer3 = torch.optim.Adam([ \n",
    "    dict(params=model3.parameters(), lr=0.0001),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create epoch runners "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model1, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model1, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_epoch_voc = smp.utils.train.TrainEpoch(\n",
    "    model2, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer2,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch_voc = smp.utils.train.ValidEpoch(\n",
    "    model2, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_epoch_voc_orig = smp.utils.train.TrainEpoch(\n",
    "    model3, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer3,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch_voc_orig = smp.utils.train.ValidEpoch(\n",
    "    model3, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0\n",
    "max_score_voc = 0\n",
    "max_score_voc_orig = 0\n",
    "\n",
    "for i in range(0, 40):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    print(\"MHP\")\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    print(\"VOC\")\n",
    "    train_logs_voc = train_epoch_voc.run(train_loader_voc)\n",
    "    valid_logs_voc = valid_epoch_voc.run(valid_loader_voc)\n",
    "   \n",
    "    print(\"VOC Orig\")\n",
    "    train_logs_voc_orig = train_epoch_voc_orig.run(train_loader_voc_orig)\n",
    "    valid_logs_voc_orig = valid_epoch_voc_orig.run(valid_loader_voc_orig)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model1, './best_model.pth')\n",
    "        print('MHP Model saved!')\n",
    "\n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')\n",
    "\n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score_voc < valid_logs_voc['iou_score']:\n",
    "        max_score_voc = valid_logs_voc['iou_score']\n",
    "        torch.save(model2, './best_model_voc.pth')\n",
    "        print('VOC Model saved!')\n",
    "    \n",
    "    if i == 25:\n",
    "        optimizer2.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score_voc_orig < valid_logs_voc_orig['iou_score']:\n",
    "        max_score_voc_orig = valid_logs_voc_orig['iou_score']\n",
    "        torch.save(model3, './best_model_voc_orig.pth')\n",
    "        print('VOC Orig Model saved!')\n",
    "        \n",
    "    if i == 25:\n",
    "        optimizer3.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "best_model = torch.load('./best_model.pth')\n",
    "best_model_voc = torch.load('./best_model_voc.pth')\n",
    "best_model_voc_orig = torch.load('./best_model_voc_orig.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, my_model):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.pretrained = my_model\n",
    "        self.my_new_layers = nn.Sequential(nn.Linear(1000, 100),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(100, 2))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(x)\n",
    "        x = self.my_new_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create test dataset\n",
    "test_dataset = Dataset(\n",
    "    size=1000,\n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomValidEpoch(smp.utils.train.ValidEpoch):\n",
    "    def __init__(self, model, loss, metrics, device='cpu', verbose=True):\n",
    "         super().__init__(\n",
    "            model=model,\n",
    "            loss=loss,\n",
    "            metrics=metrics,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    def batch_update(self, x, y):\n",
    "        with torch.no_grad():\n",
    "            prediction = self.model.forward(x)\n",
    "            # print(prediction.shape) # torch.Size([1, 21, 128, 128])\n",
    "            prediction = prediction[:,15,:,:]\n",
    "            loss = self.loss(prediction, y)\n",
    "        return loss, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# evaluate model on test set\n",
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model=best_model,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"MHP\")\n",
    "logs = test_epoch.run(test_dataloader)\n",
    "\n",
    "test_epoch_voc = smp.utils.train.ValidEpoch(\n",
    "    model=best_model_voc,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"VOC\")\n",
    "logs_voc = test_epoch_voc.run(test_dataloader)\n",
    "\n",
    "test_epoch_voc_orig = CustomValidEpoch(\n",
    "    model=best_model_voc_orig,\n",
    "    loss=loss,\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "print(\"VOC Orig\")\n",
    "logs_voc_org = test_epoch_voc_orig.run(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Time Measurement\n",
    "https://deci.ai/resources/blog/measure-inference-time-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset without transformations for image visualization\n",
    "test_dataset_vis = Dataset(\n",
    "    size=1000,\n",
    "    classes=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "    \n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    pr_mask = best_model.predict(x_tensor)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "    pr_mask_voc = best_model_voc.predict(x_tensor)\n",
    "    pr_mask_voc = (pr_mask_voc.squeeze().cpu().numpy().round())\n",
    "\n",
    "    pr_mask_voc_orig = best_model_voc_orig.predict(x_tensor)\n",
    "    pr_mask_voc_orig = (pr_mask_voc_orig.squeeze().cpu().numpy().round())\n",
    "        \n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted_mask=pr_mask,\n",
    "        predicted_mask_voc=pr_mask_voc,\n",
    "        predicted_mask_voc_orig=pr_mask_voc_orig,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "74022aae890ba747b3a4262e7e8e47cd7ef92a3925bdc86f96fbcbbd0ea6874b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
